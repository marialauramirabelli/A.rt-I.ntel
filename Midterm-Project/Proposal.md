## Midterm Project Proposal

After watching the [video](https://vimeo.com/203218419) for the CitizenScore speculative design project, I started to consider how an algorithm that is designed to interpret facial expression could be used for "the greater good" in a less-dystopian scenario than the one presented in the video. This wouldn't necessarily mean that the use of the technology would have no drawbacks, but I wanted to try to find an application of it that, at least initially, wouldn't be intended for matters such as surveillance and control.  
  
An ongoing concern on our campus is the state of its students' mental health, which is influenced by a wider interest in understanding the factors that affect mental health and the importance of tackling issues related to it. Traditionally, mental health conditions are stigmatized, but many believe the stigma is decreasing. As more individuals talk about their experiences, more diagnoses are carried out, which should supposedly be a first step in helping the people who endure these conditions. For college students in US institutions specifically, anxiety and depression are ["the most common self-reported conditions"](https://www.reuters.com/article/us-health-mental-college/mental-health-diagnoses-rising-among-u-s-college-students-idUSKCN1N65U8).  
 
NYUAD is located in Saadiyat Island, which translates to "Island of Happiness". The government of the United Arab Emirates itself has a focus on what it means to be happy and of how happiness can be systematized, with initiatives such as the appointment of a Minister of State for Happiness and the launching of the National Programme for Happiness and Positivity. Arguably, this makes NYUAD a perfect setting to try some automated technology that could help individuals struggling with mental health conditions like anxiety and depression reach an "optimum state of happiness" (as this is the goal, in a way, of the country that hosts NYUAD).  
  
An automated system would not only be able to help all individuals at once (NYUAD happens to have a shortage of counselors at some point, I'm not sure if that's still the case), but it could also identify which individuals need help before they "realize" that they do, or before they decide on their own to talk to a counselor (this could get very controversial very fast, which I'll address soon).  
  
I imagine "stands" (that look sort of like mailboxes) located around the campus, probably in places like the entrances of residential buildings or in bathrooms; not quite out in the open, but in areas that students frequent. These stands would be equipped with a camera that captures any person that walks past them, and a program that has been trained to recognize signs of emotional distress (let's call it "sadness"): tears, frowning, slouching, etc. Once "sadness" is recognized, the stand will call the individual's attention and try to cheer them up and/or give them information on how to make use of mental health resources on campus, like booking a counseling appointment (this would be done with speech synthesis). If the stands could also recognize the person they've targeted (which is not far-fetched, given that NYU has a record of all its students that includes our photographs), the program can access its own records that indicate how many times a specific person's mood ahs been analyzed, what recommendations it's given to a person before, and even if the person actually booked a counseling appointment.  
  
Even though a technology like this could have been born out of "a good cause," it's clear that it could come with several negative consequences. There's the matter of privacy, not only in terms of how publicly an individual's personal issues would be addressed, but also because of the existence of records of an individual's mental health managed by an automated system. There's a degree of control here too: this system would constantly be trying to fix "sadness," which is natural in humans. Some people allow themselves to experience their emotions when they're feeling down, and only after they do this, at their own pace, can their moods improve. How would a machine that pesters you to be happy at all times help in that process? There's the question of just how effective the aid of a machine can be in situations related to mental health. Is it more effective to talk to another person who can relate to one's condition, or who can give advice based on their experience? Does one find comfort in approaching another person with a problem because of the human interaction or could a machine very easily replace that ([the potential roles and effectiveness of AI on mental health have already been considered and explored in the field](https://www.theguardian.com/lifeandstyle/shortcuts/2019/jan/02/woebots-ai-counselling-future-therapy-mental-health))?  
  
I mainly imagine a scenario in which some people might benefit from this, but in which many others also learn how to bypass the technology and continue to cope with their conditions on their own. People could make note of where on campus the stands are located, and walk with purpose and a smile on their face whenever they approach one of them, just to switch back to their natural stance when they are out of range. This reminds me of a [video](https://vimeo.com/128873380) (titled "Uninvited Guests") I was shown in a class a couple of semesters ago, which is also about speculative design: in it, an old man gets a plethora of smart devices as gifts from his children, which are meant to provide care for him and help establish order in his life as a widower. However, he eventually decides to "trick" the devices, in order to regain his independence.  
  
If people start doing this, is the device a failure? Is it just a nuisance? Or is it successful if it manages to help at least a person or two?
