# Emotion Detector & Reward Analyzer (by Happiness Authorities)
  
*Video that demonstrates how the system works*: https://youtu.be/2oIxO5ygZq4  
  
## Concept & Installation:

This project resulted in a combination of our two midterm project ideas, one which measures the degree of "happiness" in individuals as they pass by a government-run machine (in a reality in which it's the government's priority to ensure that its citizens are content, or  alternatively, to force them to be), and the other which tackles the idea of a social credit system that restricts or grants access/privileges to citizens depending on their scores. 
  
Given that both concepts deal with surveillance and government control, it made sense to combine them and create a system that measures individuals' emotions and punishes or rewards them in return. Such a system would require characteristics such as face recognition, emotion recognition, the creation of a citizen database, and a points system. These were the features that we had to implement in our project, to create a version of our imagined system that people could interact with, and which could ideally convey the notion of a state that controls and manipulates its citizens through their emotions; this would hopefully lead users to think about what that would imply in their daily routines (this is particularly relevant because such a system is not a matter of the future, as versions of it exist today). This also leads to considering how technology can be used by the authorities, and to the question of whether or not there should be limitations in this regard.
  
The project ended up being divided into two parts, the **emotion detector** and the **reward analyzer**, both existing as a screen with a webcam each. The emotion detector detects a face, either adds it to a database (if it's a new face) or matches it with an existing one (if it's already stored in the database), and detects the person's emotion (the options range from happy to angry, from surprised to neutral, etc.). The person is then given points or has points deducted depending on their emotion ("happy" gets you the maximum extra points, "neutral" gives you none, "sad" and "angry" subtract points from your score, etc), from an original score that is assigned when the person is first recognized (this score is not the same for every individual, intending to reflect the bias in artificial intelligence regarding race, gender, etc.). The next station, the rewards analyzer, also detects faces, and if the person is found in the database, it tells them if they have a "reward" or not based on the points that the emotion detector added or deducted from their social credit score. The reward system we set up revolved around lollipops. If your score is within the highest ranges, you can get free lollipops. If it isn't, you either must pay the regular fee for a lollipop, or even pay extra money to get one.

## APIs Used:
  
As the system detected and recognized people, adding or removing points, users could see this process taking place on the screens. They could see the image that was taken of them when they passed by the webcam, their image in the database if any, and the detected emotion (with a short message referring to it - for instance, something along the lines of "good job" for "happy" or "cheer up" for "sad") or the available reward based on their score. All of this was shown on browser windows, given that our system was running on a node.js server.

The main API used was [face-api.js](https://justadudewhohacks.github.io/face-api.js/webcam_face_tracking/), which is a JavaScript face recognition API for the browser and node.js implemented on top of tensorflow.js core. This API enables a variety of functions, including face recognition in both images and videos, facial expression recognition, age estimation and gender recognition. The two functionalities we used were face recognition and facial expression (emotion) recognition. 

In the model for face recognition, a reference image can be uploaded from the computer or via an URL, and all the faces in the image will be identified. Then a query image can be uploaded, and if any of the faces in the query image appeared in the reference image also, they will be recognized, and those that did not appear in the reference image will be labeled as "unknown". For facial expression recognition, one can simply upload an image and the facial expression (emotion) of the people in that image will be recognized. 

For our project, we wanted the face recognition to be real time, so that if someone passes by the camera we set up at the detection point, his or her face and emotion can be recognized. Thus, we needed to build on top of the current face-api.js model, and make all the recognition real time. In order to achieve this goal, we used another library called [webcam.js](https://github.com/jhuckaby/webcamjs), which is simply a webcam-image capture library that captures and save images as a URI, which can be displayed on the webpage by simply attaching the URI to an img tag (HTML). As more people get detected, we are saving the image URIs in a Cloudant database, and by comparing the newly captured image with all the images saved in the database using face-api.js, we can see whether this person has shown up before. If his or her photo is already in the database, we will pull up his or her data from the database, check his or her facial expression, and change the social credit accordingly. If this person is a stranger, we save his or her image to the database, as well as the social credit score based on the facial expression detected. 

After we set up the two stations and had our project run for several days, we manually checked the images saved in our database, and tagged each person's name, so that next time when people showed up at our station, they can actually see their names being recognized. 
  
## Difficulties
  
We encountered several issues during the development of this project, ranging from the adding of the image URIs in the database to the design of the webpage. Some big obstacles had to do with understanding how to manipulate the face-api.js code in order to adapt the examples found on GitHub to our own needs, both in terms of the logic of our program but also in terms of how the features were displayed on the browser. Working with the database also posed problems, as we had certain synchronization issues, as well as difficulties with saving the information of each "citizen" and then retrieving and editing it. It was also challenging to come up with a successful way of displaying the information to the users and an interesting reward system for the lollipops (our solution involves taking a picture of the reward message displayed by the screen and showing it to Jack or Ume to get your lollipop from them, but the message has the time and date displayed on it so that you don't use the same photo more than once... it is somewhat convoluted). 
  
The project could certainly use more time for improvements, for there are several aspects of it that we could still work on. The system was not successful after we installed it and people started interacting with it. The main issues it experienced had to do with database synchronization (as aforementioned) but also with the functionality of face-api.js. For example, the API can detect faces at different angles and directions (for instance, profiles), which incurred in different people being recognized as a single one (for example, five different people being recognized as "Person 3" in the database) because all their faces where captured at the same angle, where the faces' features are not clear.
  
However, the challenge of integrating the several different features, implementing APIs we hadn't used before and learning more about coding along the way was gratifying, and this is certainly a project that would be interesting to continue to develop in the future.


