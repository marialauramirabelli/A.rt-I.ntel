## Wekinator DTW Assignment  
Video: https://youtu.be/fx9EgUmMh00   
Processing code: [Inputs & Outputs](/Wekinator-DTW-Assignment/DTW_OpticalFlow.pde)  
  
For this assignment, I used the OpenCV Optical Flow example sketch in Processing to detect movement on the screen; the vector that stores the information of this movement has two parameters (x-axis and y-axis movement), so these were the two inputs that were sent to Wekinator, which was in turn trained to capture five kinds of gestures: stationary, up-down, down-up, left-right, right-left. These gestures then triggered changes in frequency of a soundwave generated by the Minim library (I wasn't using any sound files, so I encountered no issues, unlike with the regression assignment).  
  
The training and the code work, meaning that they do what I wanted them to do; but when I showed the video of the result in class, it was brought up that the interaction is confusing, and it's hard to distinguish which sound corresponds to which movement (which I agree with, after rewatching the video). It was suggested that instead of having a constant sound that changes, I could have different sounds that are triggered separately by each of the gestures. I could have also trained other kinds of gestures, ideally some that are "more different" among each other (so that they are easier to differentiate for Wekinator) - as I trained my examples, I checked the green indicator on Weinator that lets you know if the algorithm is detecting the gesture and not confusing it with others, and it was working. But in my documentation video, the speed at which my object is moving and changing between gestures (combined with the constant sound) makes it difficult to notice the differences in outputs. As with the regression assignment, there is much that could be done to improve this one.
